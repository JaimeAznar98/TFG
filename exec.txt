1
EfficientNet(
  (features): Sequential(
    (0): ConvNormActivation(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): SiLU(inplace=True)
    )
    (1): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): ConvNormActivation(
            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (2): ConvNormActivation(
            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.0, mode=row)
      )
    )
    (2): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): ConvNormActivation(
            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): ConvNormActivation(
            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): ConvNormActivation(
            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): ConvNormActivation(
            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): ConvNormActivation(
            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): ConvNormActivation(
            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.025, mode=row)
      )
    )
    (3): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): ConvNormActivation(
            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): ConvNormActivation(
            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): ConvNormActivation(
            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): ConvNormActivation(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): ConvNormActivation(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): ConvNormActivation(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.05, mode=row)
      )
    )
    (4): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): ConvNormActivation(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): ConvNormActivation(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): ConvNormActivation(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): ConvNormActivation(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): ConvNormActivation(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): ConvNormActivation(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)
      )
      (2): MBConv(
        (block): Sequential(
          (0): ConvNormActivation(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): ConvNormActivation(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): ConvNormActivation(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)
      )
    )
    (5): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): ConvNormActivation(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): ConvNormActivation(
            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): ConvNormActivation(
            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): ConvNormActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): ConvNormActivation(
            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): ConvNormActivation(
            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)
      )
      (2): MBConv(
        (block): Sequential(
          (0): ConvNormActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): ConvNormActivation(
            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): ConvNormActivation(
            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.125, mode=row)
      )
    )
    (6): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): ConvNormActivation(
            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): ConvNormActivation(
            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)
            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): ConvNormActivation(
            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)
      )
      (1): MBConv(
        (block): Sequential(
          (0): ConvNormActivation(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): ConvNormActivation(
            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): ConvNormActivation(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)
      )
      (2): MBConv(
        (block): Sequential(
          (0): ConvNormActivation(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): ConvNormActivation(
            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): ConvNormActivation(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)
      )
      (3): MBConv(
        (block): Sequential(
          (0): ConvNormActivation(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): ConvNormActivation(
            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): ConvNormActivation(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)
      )
    )
    (7): Sequential(
      (0): MBConv(
        (block): Sequential(
          (0): ConvNormActivation(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (1): ConvNormActivation(
            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(inplace=True)
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(inplace=True)
            (scale_activation): Sigmoid()
          )
          (3): ConvNormActivation(
            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)
      )
    )
    (8): ConvNormActivation(
      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): SiLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (classifier): Sequential(
    (0): Dropout(p=0.2, inplace=True)
    (1): Linear(in_features=1280, out_features=1000, bias=True)
  )
)
1280
yes
yes
3372
5444
5434
3216
2310
cuda
loss: 1.710371  [    0/17798]
loss: 1.273871  [ 6400/17798]
loss: 1.201962  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 51.0%, Avg loss: 1.183283 

Test with train split: 
Test Error: 
 Accuracy: 52.7%, Avg loss: 1.123337 

cuda
loss: 1.091597  [    0/17798]
loss: 1.045196  [ 6400/17798]
loss: 1.037747  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 49.3%, Avg loss: 1.256122 

Test with train split: 
Test Error: 
 Accuracy: 52.6%, Avg loss: 1.153389 

cuda
loss: 1.399153  [    0/17798]
loss: 1.178351  [ 6400/17798]
loss: 1.013319  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 49.6%, Avg loss: 1.203783 

Test with train split: 
Test Error: 
 Accuracy: 53.2%, Avg loss: 1.116208 

cuda
loss: 1.025497  [    0/17798]
loss: 1.240276  [ 6400/17798]
loss: 1.364069  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 48.3%, Avg loss: 1.235708 

Test with train split: 
Test Error: 
 Accuracy: 53.8%, Avg loss: 1.123003 

cuda
loss: 0.932106  [    0/17798]
loss: 1.214057  [ 6400/17798]
loss: 1.195387  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 47.9%, Avg loss: 1.243807 

Test with train split: 
Test Error: 
 Accuracy: 52.2%, Avg loss: 1.139080 

cuda
loss: 0.873180  [    0/17798]
loss: 1.163911  [ 6400/17798]
loss: 1.312951  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 46.2%, Avg loss: 1.292014 

Test with train split: 
Test Error: 
 Accuracy: 51.9%, Avg loss: 1.158146 

cuda
loss: 1.224812  [    0/17798]
loss: 1.250702  [ 6400/17798]
loss: 1.141831  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 49.1%, Avg loss: 1.270290 

Test with train split: 
Test Error: 
 Accuracy: 51.6%, Avg loss: 1.159085 

cuda
loss: 1.232601  [    0/17798]
loss: 1.023313  [ 6400/17798]
loss: 1.321185  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 49.0%, Avg loss: 1.235569 

Test with train split: 
Test Error: 
 Accuracy: 54.3%, Avg loss: 1.117091 

cuda
loss: 0.923690  [    0/17798]
loss: 1.272327  [ 6400/17798]
loss: 1.322784  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 47.9%, Avg loss: 1.255192 

Test with train split: 
Test Error: 
 Accuracy: 52.3%, Avg loss: 1.140172 

cuda
loss: 1.004827  [    0/17798]
loss: 1.075534  [ 6400/17798]
loss: 1.274416  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 49.7%, Avg loss: 1.250768 

Test with train split: 
Test Error: 
 Accuracy: 52.8%, Avg loss: 1.140885 

=================================================
=================================================
=================================================

yes
yes
3372
5444
5434
3216
2310
cuda
loss: 1.659984  [    0/17798]
loss: 1.422984  [ 6400/17798]
loss: 1.377543  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 40.9%, Avg loss: 1.649891 

Test with train split: 
Test Error: 
 Accuracy: 43.1%, Avg loss: 1.538668 

cuda
loss: 1.667729  [    0/17798]
loss: 1.667539  [ 6400/17798]
loss: 2.000782  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 43.0%, Avg loss: 1.691576 

Test with train split: 
Test Error: 
 Accuracy: 44.0%, Avg loss: 1.600607 

cuda
loss: 1.851131  [    0/17798]
loss: 1.468359  [ 6400/17798]
loss: 1.343452  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 41.1%, Avg loss: 1.872283 

Test with train split: 
Test Error: 
 Accuracy: 41.2%, Avg loss: 1.846173 

cuda
loss: 1.888403  [    0/17798]
loss: 2.002817  [ 6400/17798]
loss: 1.287211  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 42.5%, Avg loss: 1.611098 

Test with train split: 
Test Error: 
 Accuracy: 43.7%, Avg loss: 1.569381 

cuda
loss: 1.708644  [    0/17798]
loss: 1.737275  [ 6400/17798]
loss: 1.770654  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 38.9%, Avg loss: 1.822603 

Test with train split: 
Test Error: 
 Accuracy: 39.4%, Avg loss: 1.769073 

cuda
loss: 1.806602  [    0/17798]
loss: 1.602837  [ 6400/17798]
loss: 2.147589  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 42.6%, Avg loss: 1.752196 

Test with train split: 
Test Error: 
 Accuracy: 42.8%, Avg loss: 1.704164 

cuda
loss: 1.668910  [    0/17798]
loss: 1.955594  [ 6400/17798]
loss: 1.710713  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 38.7%, Avg loss: 1.927322 

Test with train split: 
Test Error: 
 Accuracy: 37.2%, Avg loss: 1.929720 

cuda
loss: 2.252244  [    0/17798]
loss: 2.084181  [ 6400/17798]
loss: 1.604460  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 38.1%, Avg loss: 1.832630 

Test with train split: 
Test Error: 
 Accuracy: 41.1%, Avg loss: 1.714903 

cuda
loss: 1.523358  [    0/17798]
loss: 1.549443  [ 6400/17798]
loss: 1.853460  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 42.9%, Avg loss: 1.733588 

Test with train split: 
Test Error: 
 Accuracy: 42.9%, Avg loss: 1.672004 

cuda
loss: 1.876962  [    0/17798]
loss: 1.540679  [ 6400/17798]
loss: 1.816880  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 39.8%, Avg loss: 1.868445 

Test with train split: 
Test Error: 
 Accuracy: 40.8%, Avg loss: 1.767493 

=================================================
=================================================
=================================================

cuda
loss: 1.646529  [    0/17798]
loss: 1.145908  [ 6400/17798]
loss: 1.104322  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 50.3%, Avg loss: 1.161279 

Test with train split: 
Test Error: 
 Accuracy: 52.6%, Avg loss: 1.133622 

cuda
loss: 1.035751  [    0/17798]
loss: 1.218565  [ 6400/17798]
loss: 1.040219  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 51.3%, Avg loss: 1.136641 

Test with train split: 
Test Error: 
 Accuracy: 53.5%, Avg loss: 1.093357 

cuda
loss: 0.974441  [    0/17798]
loss: 1.144904  [ 6400/17798]
loss: 1.080792  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 51.1%, Avg loss: 1.158726 

Test with train split: 
Test Error: 
 Accuracy: 54.2%, Avg loss: 1.090114 

cuda
loss: 1.099020  [    0/17798]
loss: 1.024300  [ 6400/17798]
loss: 1.241771  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 51.0%, Avg loss: 1.140549 

Test with train split: 
Test Error: 
 Accuracy: 54.9%, Avg loss: 1.068962 

cuda
loss: 1.100259  [    0/17798]
loss: 1.162521  [ 6400/17798]
loss: 1.127352  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 50.3%, Avg loss: 1.148990 

Test with train split: 
Test Error: 
 Accuracy: 55.3%, Avg loss: 1.063494 

cuda
loss: 0.936837  [    0/17798]
loss: 1.105942  [ 6400/17798]
loss: 0.944922  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 50.6%, Avg loss: 1.168154 

Test with train split: 
Test Error: 
 Accuracy: 55.0%, Avg loss: 1.058777 

cuda
loss: 1.076513  [    0/17798]
loss: 1.114649  [ 6400/17798]
loss: 1.127552  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 50.7%, Avg loss: 1.160597 

Test with train split: 
Test Error: 
 Accuracy: 55.7%, Avg loss: 1.054862 

cuda
loss: 1.009576  [    0/17798]
loss: 1.159243  [ 6400/17798]
loss: 1.135890  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 50.4%, Avg loss: 1.147362 

Test with train split: 
Test Error: 
 Accuracy: 55.5%, Avg loss: 1.060886 

cuda
loss: 0.950738  [    0/17798]
loss: 1.074707  [ 6400/17798]
loss: 1.157511  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 51.6%, Avg loss: 1.120116 

Test with train split: 
Test Error: 
 Accuracy: 55.8%, Avg loss: 1.047287 

cuda
loss: 0.941889  [    0/17798]
loss: 1.083377  [ 6400/17798]
loss: 1.198058  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 50.1%, Avg loss: 1.146434 

Test with train split: 
Test Error: 
 Accuracy: 55.9%, Avg loss: 1.040768 

=================================================
=================================================
=================================================

cuda
loss: 1.619383  [    0/17798]
loss: 1.301679  [ 6400/17798]
loss: 1.060460  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 50.3%, Avg loss: 1.156911 

Test with train split: 
Test Error: 
 Accuracy: 52.1%, Avg loss: 1.133370 

cuda
loss: 1.264259  [    0/17798]
loss: 1.372765  [ 6400/17798]
loss: 1.044544  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 49.1%, Avg loss: 1.163098 

Test with train split: 
Test Error: 
 Accuracy: 54.0%, Avg loss: 1.095045 

cuda
loss: 1.207847  [    0/17798]
loss: 1.042027  [ 6400/17798]
loss: 1.240530  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 51.0%, Avg loss: 1.148292 

Test with train split: 
Test Error: 
 Accuracy: 55.0%, Avg loss: 1.073892 

cuda
loss: 1.060678  [    0/17798]
loss: 0.999814  [ 6400/17798]
loss: 1.152102  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 50.3%, Avg loss: 1.158346 

Test with train split: 
Test Error: 
 Accuracy: 54.8%, Avg loss: 1.076347 

cuda
loss: 1.069378  [    0/17798]
loss: 0.978833  [ 6400/17798]
loss: 1.167369  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 50.8%, Avg loss: 1.161390 

Test with train split: 
Test Error: 
 Accuracy: 55.1%, Avg loss: 1.062723 

cuda
loss: 1.029500  [    0/17798]
loss: 1.069659  [ 6400/17798]
loss: 1.203472  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 49.9%, Avg loss: 1.142165 

Test with train split: 
Test Error: 
 Accuracy: 56.0%, Avg loss: 1.058328 

cuda
loss: 0.919472  [    0/17798]
loss: 1.069263  [ 6400/17798]
loss: 1.286487  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 51.2%, Avg loss: 1.137255 

Test with train split: 
Test Error: 
 Accuracy: 56.0%, Avg loss: 1.050148 

cuda
loss: 0.891127  [    0/17798]
loss: 1.047665  [ 6400/17798]
loss: 1.148930  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 51.1%, Avg loss: 1.141060 

Test with train split: 
Test Error: 
 Accuracy: 55.3%, Avg loss: 1.058384 

cuda
loss: 1.134652  [    0/17798]
loss: 1.047728  [ 6400/17798]
loss: 1.195464  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 51.3%, Avg loss: 1.144667 

Test with train split: 
Test Error: 
 Accuracy: 56.0%, Avg loss: 1.051067 

cuda
loss: 0.903378  [    0/17798]
loss: 0.991753  [ 6400/17798]
loss: 0.962503  [12800/17798]
Test with test split: 
Test Error: 
 Accuracy: 51.2%, Avg loss: 1.146539 

Test with train split: 
Test Error: 
 Accuracy: 55.5%, Avg loss: 1.049227
